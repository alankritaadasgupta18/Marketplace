{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7da05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from google.oauth2.service_account import Credentials\n",
    "os.environ.get('SECRETS_PATH')\n",
    "with open('/langley/udocker/phoenix-worker/current/phoenix-worker/meehir-dsw-gcp-secret/creds') as f:\n",
    "    \n",
    "    secret_file = f.read()\n",
    "secret = eval(secret_file)\n",
    "scopes = [\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    'https://www.googleapis.com/auth/drive',\n",
    "]\n",
    "creds = Credentials.from_service_account_info(info=secret,scopes=scopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaae691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/20/2025 02:55:32 PM \u001b[92m Obtained utoken email from file \u001b[0m\n",
      "01/20/2025 02:55:33 PM \u001b[92m Obtained utoken from utoken cli \u001b[0m\n",
      "01/20/2025 02:55:33 PM \u001b[92m Obtained utoken email from file \u001b[0m\n",
      "01/20/2025 02:55:33 PM \u001b[92m Obtained utoken from utoken cli \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from querybuilder_client import QuerybuilderClient\n",
    "from queryrunner_client import Client\n",
    "import datetime\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import pygsheets\n",
    "import ast\n",
    "import IPython\n",
    "from numpy import nan\n",
    "import json\n",
    "# import pypostmaster\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import os\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "qr = Client(user_email='mrajor@ext.uber.com', consumer_name='marketplace-mobility-ops-analytics')\n",
    "\n",
    "gc = pygsheets.authorize(custom_credentials=creds)\n",
    "\n",
    "wsaccess = pygsheets.authorize(custom_credentials=creds)\n",
    "main_sheet = gc.open_by_key('1td9RLTlnKFbKGEy9mDCmerz2oZUCj-tRhc7ufch0uyY')\n",
    "details = main_sheet.worksheet_by_title('manually_handled_npi')\n",
    "qr = Client(user_email='mrajor@ext.uber.com', consumer_name='marketplace-mobility-ops-analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe05113",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report</th>\n",
       "      <th>tab_name</th>\n",
       "      <th>sheet</th>\n",
       "      <th>queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FqrD0rZqX</td>\n",
       "      <td>Uber_raw_data</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1td9RLT...</td>\n",
       "      <td>https://querybuilder.uberinternal.com/r/FqrD0r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84DjV3IUH</td>\n",
       "      <td>ola_raw_data</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1td9RLT...</td>\n",
       "      <td>https://querybuilder.uberinternal.com/r/84DjV3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FqrD0rZqX</td>\n",
       "      <td>Uber_raw_data</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/12D5K2c...</td>\n",
       "      <td>https://querybuilder.uberinternal.com/r/FqrD0r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dIpNNhMGL</td>\n",
       "      <td>rapido_raw_data</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/12D5K2c...</td>\n",
       "      <td>https://querybuilder.uberinternal.com/r/dIpNNh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Report         tab_name  \\\n",
       "0  FqrD0rZqX    Uber_raw_data   \n",
       "1  84DjV3IUH     ola_raw_data   \n",
       "2  FqrD0rZqX    Uber_raw_data   \n",
       "3  dIpNNhMGL  rapido_raw_data   \n",
       "\n",
       "                                               sheet  \\\n",
       "0  https://docs.google.com/spreadsheets/d/1td9RLT...   \n",
       "1  https://docs.google.com/spreadsheets/d/1td9RLT...   \n",
       "2  https://docs.google.com/spreadsheets/d/12D5K2c...   \n",
       "3  https://docs.google.com/spreadsheets/d/12D5K2c...   \n",
       "\n",
       "                                             queries  \n",
       "0  https://querybuilder.uberinternal.com/r/FqrD0r...  \n",
       "1  https://querybuilder.uberinternal.com/r/84DjV3...  \n",
       "2  https://querybuilder.uberinternal.com/r/FqrD0r...  \n",
       "3  https://querybuilder.uberinternal.com/r/dIpNNh...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet=details.get_as_df()\n",
    "sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "today=datetime.today()\n",
    "start_date=today-timedelta(days=1)\n",
    "end_date=today-timedelta(days=0)\n",
    "start=start_date.strftime(\"%Y-%m-%d\")\n",
    "end=end_date.strftime(\"%Y-%m-%d\")\n",
    "print(start)\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a6c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "week_number=start_date.isocalendar()[1]\n",
    "print(week_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb96eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_in_wks_replace(dash_link,tab_name,df):\n",
    "    wks=gc.open_by_url(dash_link).worksheet_by_title(tab_name)\n",
    "    wks.clear()\n",
    "    lastrow=len(wks.get_col(1,include_tailing_empty=False)) + 1\n",
    "    wks.set_dataframe(df,start='A{}'.format(lastrow),copy_head=False)\n",
    "    print(\"data uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0231003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_in_wks(dash_link,tab_name,df):\n",
    "    wks=gc.open_by_url(dash_link).worksheet_by_title(tab_name)\n",
    "    wks.add_rows(df.shape[0])\n",
    "    lastrow=len(wks.get_col(1,include_tailing_empty=False)) + 1\n",
    "    wks.set_dataframe(df,start='A{}'.format(lastrow),copy_head=False)\n",
    "    print(\"data uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37caf7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_types_common(df):\n",
    "    df['day'] = df['day'].astype(str)\n",
    "    df['Time'] = df['Time'].astype(str)\n",
    "    df['city_name'] = df['city_name'].astype(str)\n",
    "    df['scr_geo'] = df['scr_geo'].astype(str)\n",
    "    df['dest_geo'] = df['dest_geo'].astype(str)\n",
    "    df['Distance_bucket'] = df['Distance_bucket'].astype(str)\n",
    "#     df['week'] = df['week'].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eacf7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_df(uber_df, df):\n",
    "    print(\"Uber DataFrame Columns:\", uber_df.columns.tolist())\n",
    "    print(\"Input DataFrame Columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Ensure 'day' is in datetime format\n",
    "    df['day'] = pd.to_datetime(df['day'])\n",
    "    df['day'] = df['day'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Ensure numeric types for relevant columns, check if they exist\n",
    "    numeric_columns = ['sum_fare', 'sum_dist', 'points', 'requests', 'req weighted dist']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if col in uber_df.columns:\n",
    "            uber_df[col] = pd.to_numeric(uber_df[col], errors='coerce')\n",
    "\n",
    "    # Merge DataFrames\n",
    "    merged_df = pd.merge(uber_df, df, on=['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'], how='inner')\n",
    "\n",
    "    # Group and transform\n",
    "    if 'sum_fare' in merged_df.columns:\n",
    "        merged_df['sum_fare'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['sum_fare'].transform('sum')\n",
    "    if 'sum_dist' in merged_df.columns:\n",
    "        merged_df['sum_dist'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['sum_dist'].transform('sum')\n",
    "    if 'points' in merged_df.columns:\n",
    "        merged_df['points'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['points'].transform('sum')\n",
    "\n",
    "    merged_df['day'] = pd.to_datetime(merged_df['day'])\n",
    "    merged_df['week'] = merged_df['day'].dt.isocalendar().week\n",
    "\n",
    "    if 'requests' in merged_df.columns:\n",
    "        merged_df['requests'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['requests'].transform('sum')\n",
    "    if 'req weighted dist' in merged_df.columns:\n",
    "        merged_df['uber_dist'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['req weighted dist'].transform('sum')\n",
    "\n",
    "    # Convert requests to int\n",
    "    if 'requests' in merged_df.columns:\n",
    "        merged_df['requests'] = merged_df['requests'].astype(int)\n",
    "\n",
    "    # Calculate weighted fare only if columns exist\n",
    "    if 'sum_fare' in merged_df.columns and 'sum_dist' in merged_df.columns and 'uber_dist' in merged_df.columns:\n",
    "        merged_df['req weighted fare'] = (merged_df['sum_fare'] / merged_df['sum_dist']) * merged_df['uber_dist']\n",
    "    if 'requests' in merged_df.columns and 'sum_dist' in merged_df.columns:\n",
    "        merged_df['req weighted dist'] = merged_df['requests'] * merged_df['sum_dist']\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed6cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_df_rapido(uber_df, df):\n",
    "    print(\"Rapido Uber DataFrame Columns:\", uber_df.columns.tolist())\n",
    "    print(\"Input DataFrame Columns:\", df.columns.tolist())\n",
    "    \n",
    "    df['day'] = pd.to_datetime(df['day'])\n",
    "    df['day'] = df['day'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Ensure numeric types for relevant columns, check if they exist\n",
    "    numeric_columns = ['sum_fare_pre', 'sum_fare_post', 'sum_dist', 'points', 'requests', 'req weighted dist']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if col in uber_df.columns:\n",
    "            uber_df[col] = pd.to_numeric(uber_df[col], errors='coerce')\n",
    "\n",
    "    merged_df = pd.merge(uber_df, df, on=['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'], how='inner')\n",
    "\n",
    "    if 'sum_fare_pre' in merged_df.columns:\n",
    "        merged_df['sum_fare_pre'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['sum_fare_pre'].transform('sum')\n",
    "    if 'sum_fare_post' in merged_df.columns:\n",
    "        merged_df['sum_fare_post'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['sum_fare_post'].transform('sum')\n",
    "    if 'sum_dist' in merged_df.columns:\n",
    "        merged_df['sum_dist'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['sum_dist'].transform('sum')\n",
    "    if 'points' in merged_df.columns:\n",
    "        merged_df['points'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['points'].transform('sum')\n",
    "\n",
    "    #merged_df['week'] = merged_df['day'].dt.isocalendar().week\n",
    "    merged_df['week'] = week_number\n",
    "\n",
    "    if 'requests' in merged_df.columns:\n",
    "        merged_df['requests'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['requests'].transform('sum')\n",
    "    if 'req weighted dist' in merged_df.columns:\n",
    "        merged_df['uber_dist'] = merged_df.groupby(['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket'])['req weighted dist'].transform('sum')\n",
    "\n",
    "    if 'requests' in merged_df.columns:\n",
    "        merged_df['requests'] = merged_df['requests'].astype(int)\n",
    "\n",
    "    if 'sum_fare_pre' in merged_df.columns and 'sum_dist' in merged_df.columns and 'uber_dist' in merged_df.columns:\n",
    "        merged_df['req weighted fare_pre'] = (merged_df['sum_fare_pre'] / merged_df['sum_dist']) * merged_df['uber_dist']\n",
    "    if 'sum_fare_post' in merged_df.columns and 'sum_dist' in merged_df.columns and 'uber_dist' in merged_df.columns:\n",
    "        merged_df['req weighted fare_post'] = (merged_df['sum_fare_post'] / merged_df['sum_dist']) * merged_df['uber_dist']\n",
    "    \n",
    "    if 'requests' in merged_df.columns and 'sum_dist' in merged_df.columns:\n",
    "        merged_df['req weighted dist'] = merged_df['requests'] * merged_df['sum_dist']\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9071151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/20/2025 02:55:34 PM \u001b[93m Could not obtain utoken from cli: b'/bin/sh: 1: usso: not found\\n'\n",
      "Trying to obtain from file \u001b[0m\n",
      "01/20/2025 02:55:34 PM \u001b[92m Obtained utoken for user \u001b[0m\n",
      "01/20/2025 02:55:34 PM \u001b[93m Fetching metadata for Report FqrD0rZqX \u001b[0m\n",
      "01/20/2025 02:55:34 PM \u001b[92m Loaded object metadata. \u001b[0m\n",
      "01/20/2025 02:55:34 PM \u001b[93m Templating query for report FqrD0rZqX \u001b[0m\n",
      "01/20/2025 02:55:34 PM \u001b[92m Templated query successfully. \u001b[0m\n",
      "01/20/2025 02:55:34 PM Send tier_metadata {'report_id': 'FqrD0rZqX'} to Queryrunner V2.\n",
      "01/20/2025 02:55:34 PM \u001b[93m [Polling] 036d4d71-4311-4d2a-965a-d92518b18853 \u001b[0m\n",
      "01/20/2025 02:55:34 PM \u001b[93m [Status] created \u001b[0m\n",
      "01/20/2025 02:55:35 PM \u001b[93m [Status] started validation \u001b[0m\n",
      "01/20/2025 02:55:39 PM \u001b[93m [Status] started auth check \u001b[0m\n",
      "01/20/2025 02:55:43 PM \u001b[93m [Status] started waiting to execute \u001b[0m\n",
      "01/20/2025 03:00:20 PM \u001b[93m UToken has expired. Generating new utoken. \u001b[0m\n",
      "01/20/2025 03:00:20 PM \u001b[92m Obtained utoken email from file \u001b[0m\n",
      "01/20/2025 03:00:20 PM \u001b[92m Obtained utoken from utoken cli \u001b[0m\n",
      "01/20/2025 03:03:20 PM \u001b[93m UToken has expired. Generating new utoken. \u001b[0m\n",
      "01/20/2025 03:03:20 PM \u001b[92m Obtained utoken email from file \u001b[0m\n",
      "01/20/2025 03:03:20 PM \u001b[92m Obtained utoken from utoken cli \u001b[0m\n",
      "01/20/2025 03:08:21 PM \u001b[93m UToken has expired. Generating new utoken. \u001b[0m\n",
      "01/20/2025 03:08:21 PM \u001b[92m Obtained utoken email from file \u001b[0m\n",
      "01/20/2025 03:08:21 PM \u001b[92m Obtained utoken from utoken cli \u001b[0m\n",
      "01/20/2025 03:13:22 PM \u001b[93m UToken has expired. Generating new utoken. \u001b[0m\n",
      "01/20/2025 03:13:22 PM \u001b[92m Obtained utoken email from file \u001b[0m\n",
      "01/20/2025 03:13:22 PM \u001b[92m Obtained utoken from utoken cli \u001b[0m\n",
      "01/20/2025 03:18:23 PM \u001b[93m UToken has expired. Generating new utoken. \u001b[0m\n",
      "01/20/2025 03:18:23 PM \u001b[92m Obtained utoken email from file \u001b[0m\n",
      "01/20/2025 03:18:23 PM \u001b[92m Obtained utoken from utoken cli \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Main Logic\n",
    "ola_uber_df = pd.DataFrame()\n",
    "app_df = pd.DataFrame()\n",
    "rapido_uber_df = pd.DataFrame()\n",
    "moto_uber_df_web = pd.DataFrame()\n",
    "moto_uber_df_app = pd.DataFrame()\n",
    "\n",
    "for i in range(0,4):\n",
    "    df = qr.execute_report(sheet.iloc[i, 0], parameters={\"start\": start, \"end\": end}, datacenter='phx2').to_pandas()\n",
    "#     try:\n",
    "#         df = qr.execute_report(sheet.iloc[i, 0], parameters={\"start\": start, \"end\": end}, datacenter='phx2').to_pandas()\n",
    "    \n",
    "#     # Add delay bw iteration\n",
    "#         if i <4: # don't sleep after last iteration\n",
    "#             time.sleep(60)\n",
    "#     except Exception as e:\n",
    "#         if \"DuplicateQuery\" in str(e):\n",
    "#             time.sleep(10) #wait 10 seconds before retry\n",
    "#             df = qr.execute_report(sheet.iloc[i, 0], parameters={\"start\": start, \"end\": end}, datacenter='phx2').to_pandas()\n",
    "#         raise e\n",
    "    if sheet.iloc[i, 1] == 'Uber_raw_data':\n",
    "        df['day'] = pd.to_datetime(df['day'])\n",
    "        df['week'] = df['day'].dt.isocalendar().week\n",
    "        df['req weighted fare'] = df['sum_fare']\n",
    "        df['req weighted dist'] = df['sum_dist']\n",
    "        ola_uber_df = df\n",
    "        rapido_uber_df = df\n",
    "        app_df = df\n",
    "        upload_in_wks(sheet.iloc[i, 2], sheet.iloc[i, 1], df) \n",
    "\n",
    "        \n",
    "    elif sheet.iloc[i, 1] == 'ola_raw_data':\n",
    "        print(app_df)\n",
    "        app_df.drop(columns=['sum_fare', 'sum_dist', 'points', 'req weighted fare', 'week'], inplace=True)\n",
    "        app_df = make_data_types_common(app_df)\n",
    "        df = make_data_types_common(df)\n",
    "        new_df = merging_df(app_df, df)\n",
    "        temp_df = new_df[['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket',\n",
    "                           'sum_fare', 'sum_dist', 'points', 'week', 'requests',\n",
    "                           'req weighted fare', 'uber_dist', 'req weighted dist']]\n",
    "        upload_in_wks(sheet.iloc[i, 2], sheet.iloc[i, 1], temp_df)  \n",
    "        \n",
    "    elif sheet.iloc[i,1]=='rapido_raw_data':\n",
    "        print(rapido_uber_df.columns)\n",
    "        rapido_uber_df=rapido_uber_df[['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket',\n",
    "       'requests', 'req weighted dist']]\n",
    "        rapido_uber_df=make_data_types_common(rapido_uber_df)\n",
    "        df=make_data_types_common(df)\n",
    "        rapido_new_df=merging_df_rapido(rapido_uber_df,df)\n",
    "        rapido_temp_df = rapido_new_df[['day', 'Time', 'city_name', 'scr_geo', 'dest_geo', 'Distance_bucket',\n",
    "       'sum_fare_pre','sum_fare_post', 'sum_dist', 'points', 'week', 'requests',\n",
    "       'req weighted fare_pre','req weighted fare_post', 'uber_dist', 'req weighted dist']]\n",
    "        upload_in_wks(sheet.iloc[i,2],sheet.iloc[i,1],rapido_temp_df)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc39d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03. Python 3.7 (General DS)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
